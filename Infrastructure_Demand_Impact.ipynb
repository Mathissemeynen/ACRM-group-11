{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62995792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5945e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import facilities dataset\n",
    "fac = pd.read_csv(\"./Data/facilities.csv\")\n",
    "fac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e2ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of missing values per variable\n",
    "for col in fac.columns:\n",
    "    missings = len(fac[col][fac[col].isnull()]) / float(len(fac))\n",
    "    print(col, missings)\n",
    "\n",
    "# All sales_* columns have a very high number of missing values indicating a data quality issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbe6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further investigation of missing values in facilities columns\n",
    "\n",
    "# we will exclude sales_* (because of DQ issue) and determine which columns are metadata\n",
    "exclude_prefix = 'sales_'\n",
    "metadata = {'station_id', 'station', 'name', 'street', 'zip', 'city'}\n",
    "cols_to_check = [\n",
    "    c for c in fac.columns\n",
    "    if not c.startswith(exclude_prefix) and c not in metadata\n",
    "]\n",
    "\n",
    "print(f\"Number of columns checked: {len(cols_to_check)}\")\n",
    "print(\"Some of the checked columns:\", cols_to_check[:12])\n",
    "\n",
    "# make mask for rows with at least one NaN in those columns\n",
    "nan_mask = fac[cols_to_check].isna().any(axis=1)\n",
    "\n",
    "# make results table\n",
    "id_cols = [c for c in ['station_id','station','name'] if c in fac.columns]\n",
    "result_cols = id_cols + cols_to_check\n",
    "\n",
    "nan_stations = fac.loc[nan_mask, result_cols].copy()\n",
    "\n",
    "# add column to table that counts how many NaN's there are per station for the checked columns\n",
    "nan_stations['n_missing'] = fac.loc[nan_mask, cols_to_check].isna().sum(axis=1)\n",
    "\n",
    "nan_per_column = nan_stations[cols_to_check].isna().sum().sort_values(ascending=False).to_frame(name='n_missing_in_subset')\n",
    "\n",
    "# overview\n",
    "print(\"Number of stations with at least 1 NaN:\", nan_stations.shape[0])\n",
    "display(nan_stations.head(50)) \n",
    "display(nan_per_column)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41507d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the above, we see that the stations with missing facility data contain no information in all facility columns.\n",
    "# Therefore, we will remove all stations with missing facility data from the dataset to perform our analysis.\n",
    "# Additionally, we exclude all sales_* columns from the resulting dataframe.\n",
    "sales_cols = [col for col in fac.columns if col.startswith('sales_')]\n",
    "cols_to_keep = [col for col in fac.columns if col not in sales_cols]\n",
    "fac_no_missing_facilities = fac.loc[~nan_mask, cols_to_keep].copy()\n",
    "fac_no_missing_facilities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae3ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We transform disabled_parking_spots to a binary indicator\n",
    "fac_no_missing_facilities[\"disabled_parking_ind\"] = (fac[\"disabled_parking_spots\"].fillna(0) > 0).astype(float)\n",
    "\n",
    "# Binary columns\n",
    "binary_cols = [\"ticket_vending_machine\",\"luggage_lockers\",\"free_parking\",\"taxi\",\"bicycle_spots\",\"blue-bike\",\"bus\",\"tram\",\"metro\",\"wheelchair_available\",\"ramp\",\"disabled_parking_ind\",\"elevated_platform\",\"escalator_up\",\"escalator_down\",\"elevator_platform\",\"audio_induction_loop\"]\n",
    "\n",
    "# Sum over binary columns + disabled_parking_ind\n",
    "fac_no_missing_facilities[\"n_facilities\"] = fac_no_missing_facilities[binary_cols].sum(axis=1)\n",
    "fac_no_missing_facilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d1ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import travelers dataset\n",
    "travelers = pd.read_csv(\"./Data/travelers.csv\", sep=\";\", index_col=0)\n",
    "# Rename for convenience\n",
    "travelers = travelers.rename({\"Station\": \"station\",\n",
    "                                    \"Avg number of travelers in the week\": \"week\",\n",
    "                                    \"Avg number of travelers on Saturday\": \"saturday\",\n",
    "                                    \"Avg number of travelers on Sunday\": \"sunday\"}, axis=1)\n",
    "travelers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aec9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of missing values per variable\n",
    "for col in travelers.columns:\n",
    "    missings = len(travelers[col][travelers[col].isnull()]) / float(len(travelers))\n",
    "    print(col, missings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f68377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further inspection on Wikipedia and the NMBS website reveal that there are no train rides on these dates for these stations. \n",
    "# For example, Baasrode-Zuid & Buda only have train rides during the week and none in the weekend. \n",
    "# Therefore, we will impute every missing value with zero.\n",
    "travelers['week'] = travelers['week'].fillna(0)\n",
    "travelers['saturday'] = travelers['saturday'].fillna(0)\n",
    "travelers['sunday'] = travelers['sunday'].fillna(0)\n",
    "\n",
    "# Show\n",
    "travelers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create total travelers over the week\n",
    "travelers[\"week_total\"] = 5 * travelers[\"week\"] + travelers[\"saturday\"] + travelers[\"sunday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get avg travelers per day (including weekends)\n",
    "travelers[\"avg_day\"] = travelers[\"week_total\"] / float(7)\n",
    "travelers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stations dataset\n",
    "stations = pd.read_csv(\"./Data/stations.csv\")\n",
    "stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ca289",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations[\"daily_trains\"] = stations[\"avg_stop_times\"]\n",
    "stations[\"daily_trains\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65637785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets on station_id and name\n",
    "from unidecode import unidecode\n",
    "fac_no_missing_facilities['name_clean'] = fac_no_missing_facilities['name'].apply(lambda x: unidecode(str(x)).lower().strip())\n",
    "travelers['station_clean'] = travelers['station'].apply(lambda x: unidecode(str(x)).lower().strip())\n",
    "df = fac_no_missing_facilities.merge(stations[['station_id', 'daily_trains']], on='station_id', how='left')\n",
    "df = df.merge(travelers[['station_clean', 'avg_day']],\n",
    "              left_on='name_clean', right_on='station_clean', how='left')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7585ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression: number of facilities on number of trains and number of travelers\n",
    "reg_df = df[['n_facilities', 'daily_trains', 'avg_day']].dropna()\n",
    "X = reg_df[['daily_trains', 'avg_day']]\n",
    "y = reg_df['n_facilities']\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695af8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents = pd.read_csv(\"./Data/incidents.csv\", sep=\";\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd25cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all rows where 'place' is '-'\n",
    "dash_rows = incidents[incidents['Place'] == \"-\"]\n",
    "display(dash_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9605e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate columns 'Place.1' en 'Place.2'\n",
    "if 'Place.1' in incidents.columns:\n",
    "    incidents = incidents.drop(columns=['Place.1'])\n",
    "if 'Place.2' in incidents.columns:\n",
    "    incidents = incidents.drop(columns=['Place.2'])\n",
    "incidents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0254f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value analysis in incidents dataset\n",
    "for col in incidents.columns:\n",
    "    missings = len(incidents[col][incidents[col].isnull()]) / float(len(incidents))\n",
    "    print(col, missings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb74688",
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents.columns = incidents.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda71bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold for extreme delays\n",
    "threshold = incidents['minutes of delay'].quantile(0.9)\n",
    "extreme_incidents = incidents[incidents['minutes of delay'] >= threshold]\n",
    "extreme_incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62520c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of extreme delays by incident type\n",
    "type_summary = (extreme_incidents.groupby('incident description', as_index = False).agg(n_extreme=('minutes of delay', 'count'),\n",
    "        avg_delay=('minutes of delay', 'mean'),\n",
    "        total_delay=('minutes of delay', 'sum'),\n",
    "        avg_cancellations=('number of cancelled trains', 'mean')).sort_values('total_delay', ascending = False))\n",
    "display(type_summary.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0492af49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>place</th>\n",
       "      <th>n_extreme</th>\n",
       "      <th>avg_delay</th>\n",
       "      <th>total_cancelled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>13</td>\n",
       "      <td>15472.538462</td>\n",
       "      <td>6801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BRUSSEL-NOORD</td>\n",
       "      <td>6</td>\n",
       "      <td>10166.666667</td>\n",
       "      <td>1626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANDERLECHT</td>\n",
       "      <td>1</td>\n",
       "      <td>9671.000000</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BRUSSEL-CENTRAAL</td>\n",
       "      <td>5</td>\n",
       "      <td>7283.800000</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GEMBLOUX</td>\n",
       "      <td>3</td>\n",
       "      <td>6692.000000</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AALTER</td>\n",
       "      <td>1</td>\n",
       "      <td>6689.000000</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ANTWERPEN-CENTRAAL</td>\n",
       "      <td>1</td>\n",
       "      <td>6640.000000</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANTWERPEN-BERCHEM</td>\n",
       "      <td>1</td>\n",
       "      <td>6622.000000</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>VELTEM</td>\n",
       "      <td>1</td>\n",
       "      <td>6528.000000</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ZAVENTEM</td>\n",
       "      <td>2</td>\n",
       "      <td>6026.000000</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 place  n_extreme     avg_delay  total_cancelled\n",
       "0                    -         13  15472.538462             6801\n",
       "9        BRUSSEL-NOORD          6  10166.666667             1626\n",
       "2           ANDERLECHT          1   9671.000000              207\n",
       "7     BRUSSEL-CENTRAAL          5   7283.800000              657\n",
       "16            GEMBLOUX          3   6692.000000              503\n",
       "1               AALTER          1   6689.000000              207\n",
       "5   ANTWERPEN-CENTRAAL          1   6640.000000              154\n",
       "4    ANTWERPEN-BERCHEM          1   6622.000000              124\n",
       "34              VELTEM          1   6528.000000              171\n",
       "40            ZAVENTEM          2   6026.000000              238"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analysis of extreme delays by location by looking at incident total delay \n",
    "location_summary = (extreme_incidents.groupby('place', as_index=False)\n",
    "    .agg(\n",
    "        n_extreme=('minutes of delay', 'count'),\n",
    "        avg_delay=('minutes of delay', 'mean'),\n",
    "        total_cancelled=('number of cancelled trains', 'sum')\n",
    "    )\n",
    "    .sort_values('avg_delay', ascending=False)\n",
    ")\n",
    "\n",
    "display(location_summary.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443fb714",
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_incidents['place_clean'] = extreme_incidents['place'].str.lower().str.strip()\n",
    "travelers['station_clean'] = travelers['station'].str.lower().str.strip()\n",
    "\n",
    "impact = extreme_incidents.merge(\n",
    "    travelers[['station_clean', 'week_total']],\n",
    "    left_on='place_clean', right_on='station_clean', how='left'\n",
    ")\n",
    "\n",
    "impact[\"affected_passengers\"] = impact[\"week_total\"]\n",
    "\n",
    "impact_summary = (\n",
    "    impact.groupby('place', as_index=False)\n",
    "    .agg(\n",
    "        n_extreme=('minutes of delay', 'count'),\n",
    "        avg_delay=('minutes of delay', 'mean'),\n",
    "        total_delay=('minutes of delay', 'sum'),\n",
    "        total_cancelled=('number of cancelled trains', 'sum'),\n",
    "        avg_travelers=('affected_passengers', 'mean')\n",
    "    )\n",
    ")\n",
    "\n",
    "# ImpactScore ​= Totaal aantal minuten vertraging op station i × Gemiddeld aantal reizigers op station i\n",
    "# Toont waar veel reizigers kunnen worden beïnvloed door vertragingen.\n",
    "impact_summary['passenger_delay_index'] = impact_summary['total_delay'] * impact_summary['avg_travelers']\n",
    "\n",
    "impact_summary = impact_summary.sort_values('passenger_delay_index', ascending=False)\n",
    "\n",
    "display(impact_summary.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4df61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top = priority.head(10)\n",
    "plt.barh(top['place'], top['passenger_delay_index'])\n",
    "plt.xlabel('Passenger Delay Index (delay × travelers)')\n",
    "plt.ylabel('Station')\n",
    "plt.title('Top 10 Stations with biggest impact on travelers in case of extreme delays')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
